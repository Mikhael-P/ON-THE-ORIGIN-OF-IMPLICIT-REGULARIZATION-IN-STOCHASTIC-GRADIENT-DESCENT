{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implicit Regularisation_KIBINDA",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENvaSjI1rWMO"
      },
      "source": [
        "**1. How deep learning works at all**\n",
        "\n",
        "To figure out how deep learning works, it's not sufficient to focus on the loss function or the model class only. During training step, many algorithms are used to find minima namely: Gradient Descente (GD), Mini-Batch Gradient Descente (MBGD) or Stochastic Gradient Descente (SGD). But SGD seems to play an important role.\n",
        "\n",
        "For the specific training data, there are several minima, some of them generalise well (ie result in low test error) others can be arbitrarly badly overfit.\n",
        "\n",
        "In this kind of situation, one is interested in whether the optimisation algorithm converges quickly to a local minimum (such as a general principle) but here the interest is in which of the available minima it prefers to reach first. \n",
        "\n",
        "Optimisation algorithms have certain preference in their convergence to a minimum among the possible available minima and this preference is often described as an \"implicit regularization\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A2S0tNnxml7"
      },
      "source": [
        "**2. State of the art**\n",
        "\n",
        "When we train the deep learning model, the learning rate plays in some case an important role. Managing the learning rate can significantly achieve the performance both in test and train accuracies. Large learning rate can give the high test accuracy and this effect can minimize the training loss. It's often difficult to generalize this phenomenom, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss.\n",
        "\n",
        "To interpret this phenomenom, we use the SGD by modifying the loss function. \n",
        "\n",
        "This modification consists to combine the original loss function with an implicit regularizer which penalizes the norms of the Mini-Batch Gradients. All this modification is done under some assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size.\n",
        "\n",
        "The modified loss is:\n",
        "\n",
        "$$C_{modSGD}(w) = C_{Org}(w) + \\dfrac{\\epsilon}{4m}\\displaystyle\\sum_{j = 1}^{m}\\|{\\nabla C_j(w)}\\|^2$$\n",
        "\n",
        "Where $m = \\dfrac{N}{B}$ defines the number of batches per epoch, B: batch size and N: The training set size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxfQzhWxAM3P"
      },
      "source": [
        "**3. Core idea**\n",
        "\n",
        "Modified the loss function in order to see how the small and finite learning rate can aid generalization.\n",
        "\n",
        "This novel approach is called **implicit regularization**, this technique is established for analysing optimization algorithms (especially SGD) with finite and small step or learning rate. SGD with random shuffling, iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKjorrwEDDO"
      },
      "source": [
        "**4. How to approach the paper in term of the implementation ?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RugXnh1UrUeK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI9cpExI8U9l",
        "outputId": "364d6870-dd27-427b-ee84-811297563a52"
      },
      "source": [
        "!pip install mxnet\n",
        "from __future__ import print_function\n",
        "import mxnet as mx\n",
        "import mxnet.ndarray as nd\n",
        "from mxnet import nd, autograd, gluon\n",
        "import numpy as np\n",
        "ctx = mx.cpu()\n",
        "mx.random.seed(1)\n",
        "\n",
        "\n",
        "# for plotting purposes\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (1.8.0.post0)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet) (0.8.4)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo1w0fS5-_j5"
      },
      "source": [
        "batch_size = 64\n",
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "def transform(data, label):\n",
        "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
        "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
        "                                      batch_size, shuffle=True)\n",
        "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
        "                                     batch_size, shuffle=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mza1ABM4_VSi"
      },
      "source": [
        "# Parameters\n",
        "\n",
        "#######################\n",
        "#  Set the scale for weight initialization and choose\n",
        "#  the number of hidden units in the fully-connected layer\n",
        "#######################\n",
        "\n",
        "weight_scale = .01\n",
        "num_fc = 128\n",
        "num_filter_conv_layer1 = 20\n",
        "num_filter_conv_layer2 = 50\n",
        "\n",
        "W1 = nd.random_normal(shape=(num_filter_conv_layer1, 1, 3,3), scale=weight_scale, ctx=ctx)\n",
        "b1 = nd.random_normal(shape=num_filter_conv_layer1, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W2 = nd.random_normal(shape=(num_filter_conv_layer2, num_filter_conv_layer1, 5, 5),\n",
        "                                                    scale=weight_scale, ctx=ctx)\n",
        "b2 = nd.random_normal(shape=num_filter_conv_layer2, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W3 = nd.random_normal(shape=(800, num_fc), scale=weight_scale, ctx=ctx)\n",
        "b3 = nd.random_normal(shape=num_fc, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W4 = nd.random_normal(shape=(num_fc, num_outputs), scale=weight_scale, ctx=ctx)\n",
        "b4 = nd.random_normal(shape=num_outputs, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "params = [W1, b1, W2, b2, W3, b3, W4, b4]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LZWpWuG_VOC"
      },
      "source": [
        "for param in params:\n",
        "    param.attach_grad()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxXYqLg6_37q",
        "outputId": "895a4bdf-eac3-4959-fc6f-dc479a9b4b03"
      },
      "source": [
        "\n",
        "for data, _ in train_data:\n",
        "    data = data.as_in_context(ctx)\n",
        "    break\n",
        "conv = nd.Convolution(data=data, weight=W1, bias=b1, kernel=(3,3), num_filter=num_filter_conv_layer1)\n",
        "print(conv.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 20, 26, 26)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ2JH3Yl_34M",
        "outputId": "ebd563ab-38f4-4c90-88c2-a9754932ef97"
      },
      "source": [
        "# Average pooling\n",
        "\n",
        "pool = nd.Pooling(data=conv, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
        "print(pool.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 20, 13, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ87gmvR_30b"
      },
      "source": [
        "# Activation function\n",
        "\n",
        "def relu(X):\n",
        "    return nd.maximum(X,nd.zeros_like(X))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZNGTlpO_3wN"
      },
      "source": [
        "# Softmax output\n",
        "\n",
        "def softmax(y_linear):\n",
        "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
        "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
        "    return exp / partition"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5hH91iA830q"
      },
      "source": [
        "# Softmax cross-entropy loss\n",
        "\n",
        "def softmax_cross_entropy(yhat_linear, y):\n",
        "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecbo2saUv9h8"
      },
      "source": [
        "# # Model\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "#         self.fc2 = nn.Linear(120, 84)\n",
        "#         self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = x.view(-1, 16 * 5 * 5)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# net = Net()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whT9TLQyLx6w"
      },
      "source": [
        "# model\n",
        "\n",
        "def net(X, debug=False):\n",
        "    ########################\n",
        "    #  Define the computation of the first convolutional layer\n",
        "    ########################\n",
        "    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=(3,3),\n",
        "                                  num_filter=num_filter_conv_layer1)\n",
        "    h1_activation = relu(h1_conv)\n",
        "    h1 = nd.Pooling(data=h1_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
        "    if debug:\n",
        "        print(\"h1 shape: %s\" % (np.array(h1.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Define the computation of the second convolutional layer\n",
        "    ########################\n",
        "    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=(5,5),\n",
        "                                  num_filter=num_filter_conv_layer2)\n",
        "    h2_activation = relu(h2_conv)\n",
        "    h2 = nd.Pooling(data=h2_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
        "    if debug:\n",
        "        print(\"h2 shape: %s\" % (np.array(h2.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Flattening h2 so that we can feed it into a fully-connected layer\n",
        "    ########################\n",
        "    h2 = nd.flatten(h2)\n",
        "    if debug:\n",
        "        print(\"Flat h2 shape: %s\" % (np.array(h2.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Define the computation of the third (fully-connected) layer\n",
        "    ########################\n",
        "    h3_linear = nd.dot(h2, W3) + b3\n",
        "    h3 = relu(h3_linear)\n",
        "    if debug:\n",
        "        print(\"h3 shape: %s\" % (np.array(h3.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Define the computation of the output layer\n",
        "    ########################\n",
        "    yhat_linear = nd.dot(h3, W4) + b4\n",
        "    #yhat_linear = softmax(yhat_linear)\n",
        "    if debug:\n",
        "        print(\"yhat_linear shape: %s\" % (np.array(yhat_linear.shape)))\n",
        "\n",
        "    return yhat_linear"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-hegPsOM2am",
        "outputId": "3f16d200-717b-4a62-e18e-18ac04e9c96e"
      },
      "source": [
        "# Test Run \n",
        "\n",
        "output = net(data, debug=True)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h1 shape: [64 20 13 13]\n",
            "h2 shape: [64 50  4  4]\n",
            "Flat h2 shape: [ 64 800]\n",
            "h3 shape: [ 64 128]\n",
            "yhat_linear shape: [64 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl0X4R_jM_k9"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccD8pXnANOuP"
      },
      "source": [
        "# The Metric Evaluation\n",
        "\n",
        "def evaluate_accuracy(data_iterator, net):\n",
        "    numerator = 0.\n",
        "    denominator = 0.\n",
        "    loss_avg = 0.\n",
        "    for i, (data, label) in enumerate(data_iterator):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        label_one_hot = nd.one_hot(label, 10)\n",
        "        output = net(data)\n",
        "        predictions = nd.argmax(output, axis=1)\n",
        "        numerator += nd.sum(predictions == label)\n",
        "        denominator += data.shape[0]\n",
        "    return (numerator / denominator).asscalar()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0LnBqLCOWFq"
      },
      "source": [
        "# def evaluate_accuracy(data_iterator, net):\n",
        "#     numerator = 0.\n",
        "#     denominator = 0.\n",
        "#     loss_avg = 0.\n",
        "#     for i, (data, label) in enumerate(data_iterator):\n",
        "#         data = data.as_in_context(ctx).reshape((-1,784))\n",
        "#         label = label.as_in_context(ctx)\n",
        "#         label_one_hot = nd.one_hot(label, 10)\n",
        "#         output = net(data)\n",
        "#         loss = cross_entropy(output, label_one_hot)\n",
        "#         predictions = nd.argmax(output, axis=1)\n",
        "#         numerator += nd.sum(predictions == label)\n",
        "#         denominator += data.shape[0]\n",
        "#         loss_avg = loss_avg*i/(i+1) + nd.mean(loss).asscalar()/(i+1)\n",
        "#     return (numerator / denominator).asscalar(), loss_avg\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98u-IFKBNVyl",
        "outputId": "bf3c5e6e-714c-4620-8016-8474783cb950"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "epochs = 1\n",
        "learning_rate = .01\n",
        "smoothing_constant = .01\n",
        "\n",
        "for e in range(epochs):\n",
        "    for i, (data, label) in enumerate(train_data):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        label_one_hot = nd.one_hot(label, num_outputs)\n",
        "        with autograd.record():\n",
        "            output = net(data)\n",
        "            loss = softmax_cross_entropy(output, label_one_hot)\n",
        "        loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        curr_loss = nd.mean(loss).asscalar()\n",
        "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
        "\n",
        "\n",
        "    test_accuracy = evaluate_accuracy(test_data, net)\n",
        "    train_accuracy = evaluate_accuracy(train_data, net)\n",
        "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 0.16396560367562274, Train_acc 0.95383334, Test_acc 0.959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiQOMUD6U1rn"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-szI0O2rWUKs"
      },
      "source": [
        "*Implicit regularization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmHFRZlEp-tQ"
      },
      "source": [
        "# implicit regularization\n",
        "\n",
        "batch = len(train_data) / batch_size\n",
        "\n",
        "def l2_penalty(params):\n",
        "    penalty = nd.zeros(shape=1)\n",
        "    for param in params:\n",
        "        penalty = penalty + (1 / 4 * batch) * nd.sum(nd.norm(param.grad, ord = 2) ** 2)\n",
        "    return penalty"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSIFFwdPU2QD"
      },
      "source": [
        "# for param in params:\n",
        "#     param[:] = nd.random_normal(shape=param.shape)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ5T0-d_Vetu",
        "outputId": "15316000-8f3c-4247-ad3c-1f6a6f7e7485"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 2e-2\n",
        "smoothing_constant = .01\n",
        "#l2_strength = learning_rate / (4 * batch_size)\n",
        "l2_strength = 2e-2\n",
        "\n",
        "for e in range(epochs):\n",
        "    for i, (data, label) in enumerate(train_data):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        label_one_hot = nd.one_hot(label, num_outputs)\n",
        "        with autograd.record():\n",
        "            output = net(data)\n",
        "            loss = softmax_cross_entropy(output, label_one_hot) + l2_strength * l2_penalty(params)\n",
        "        loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        curr_loss = nd.mean(loss).asscalar()\n",
        "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
        "\n",
        "\n",
        "    test_accuracy = evaluate_accuracy(test_data, net)\n",
        "    train_accuracy = evaluate_accuracy(train_data, net)\n",
        "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 33.35400507958277, Train_acc 0.11236667, Test_acc 0.1135\n",
            "Epoch 1. Loss: 34.8734357119291, Train_acc 0.09871667, Test_acc 0.098\n",
            "Epoch 2. Loss: 34.24208737771534, Train_acc 0.09751666, Test_acc 0.0974\n",
            "Epoch 3. Loss: 33.67863074317033, Train_acc 0.09751666, Test_acc 0.0974\n",
            "Epoch 4. Loss: 34.30400169363496, Train_acc 0.11236667, Test_acc 0.1135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_omHrqxLBbnj"
      },
      "source": [
        ""
      ],
      "execution_count": 35,
      "outputs": []
    }
  ]
}